A la hora de realizar la ingesta de datos, tendría en cuenta los siguientes parámetros, ya que su configuración va a estar ligada básicamente al consumo de fuente y rendimiento de Spark: 

- spark.executor.instances: Número de ejecutores de la aplicación Spark .
- spark.executor.memory: Cantidad de memoria a usar para cada ejecutor que ejecuta la tarea.
- spark.executor.cores: Número de tareas simultáneas que puede ejecutar un ejecutor.
- spark.driver.memory: cantidad de memoria que se utilizará para el controlador.
- spark.sql.shuffle.partitions: número de particiones que se utilizarán al barajar datos para combinaciones o agregaciones.

Los resultados los escribiría en Parquet, porque es un formato de comprensión que almacena sus datos en columna, esta característica resulta muy útil ya que vamos a necesitar 
acceder a subconjuntos de datos para agrupar la información (por user, segment, session, etc) y no es necesario leer la fila completa (como en el caso de Avro).
Este formato nos va a permitir acceder rápidamente a la información que necesitamos. 
